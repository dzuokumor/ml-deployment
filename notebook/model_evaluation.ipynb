{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Land Cover Classification Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('model/land_cover_model.h5')\n",
    "print(f'model loaded: {model.input_shape} -> {model.output_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['bare_sparse', 'built_up', 'cropland', 'grassland', 'mangroves', 'shrubland', 'trees', 'water', 'wetland']\n",
    "img_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = Path('data/training_data')\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "for idx, class_name in enumerate(class_names):\n",
    "    class_path = test_data_path / class_name\n",
    "    if class_path.exists():\n",
    "        image_files = list(class_path.glob('*.png')) + list(class_path.glob('*.jpg'))\n",
    "        for img_path in image_files[:50]:\n",
    "            img = keras.preprocessing.image.load_img(img_path, target_size=(img_size, img_size))\n",
    "            img_array = keras.preprocessing.image.img_to_array(img) / 255.0\n",
    "            test_images.append(img_array)\n",
    "            test_labels.append(idx)\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)\n",
    "print(f'loaded {len(test_images)} test images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images, verbose=0)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "confidence_scores = np.max(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(test_labels, predicted_classes)\n",
    "print(f'overall accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, predicted_classes, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels, predicted_classes)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names, cbar_kws={'label': 'accuracy'})\n",
    "plt.title('confusion matrix (normalized)')\n",
    "plt.ylabel('true class')\n",
    "plt.xlabel('predicted class')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class-wise Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_accuracies = cm_normalized.diagonal()\n",
    "class_performance = list(zip(class_names, class_accuracies))\n",
    "class_performance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for class_name, acc in class_performance:\n",
    "    print(f'{class_name:15} {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh([name for name, _ in class_performance], [acc for _, acc in class_performance], color='#2196f3')\n",
    "plt.xlabel('accuracy')\n",
    "plt.title('per-class accuracy')\n",
    "plt.xlim(0, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(confidence_scores, bins=50, color='#9c27b0', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('confidence score')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('prediction confidence distribution')\n",
    "plt.axvline(confidence_scores.mean(), color='red', linestyle='--', label=f'mean: {confidence_scores.mean():.3f}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_mask = predicted_classes == test_labels\n",
    "correct_confidence = confidence_scores[correct_mask]\n",
    "incorrect_confidence = confidence_scores[~correct_mask]\n",
    "\n",
    "print(f'correct predictions: {len(correct_confidence)} (avg confidence: {correct_confidence.mean():.4f})')\n",
    "print(f'incorrect predictions: {len(incorrect_confidence)} (avg confidence: {incorrect_confidence.mean():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = np.random.choice(len(test_images), 12, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(15, 11))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, sample_idx in enumerate(sample_indices):\n",
    "    ax = axes[idx]\n",
    "    ax.imshow(test_images[sample_idx])\n",
    "    \n",
    "    true_class = class_names[test_labels[sample_idx]]\n",
    "    pred_class = class_names[predicted_classes[sample_idx]]\n",
    "    conf = confidence_scores[sample_idx]\n",
    "    \n",
    "    is_correct = test_labels[sample_idx] == predicted_classes[sample_idx]\n",
    "    color = 'green' if is_correct else 'red'\n",
    "    \n",
    "    ax.set_title(f'true: {true_class}\\npred: {pred_class}\\nconf: {conf:.3f}', \n",
    "                 color=color, fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_indices = np.where(predicted_classes != test_labels)[0]\n",
    "print(f'total misclassifications: {len(misclassified_indices)}')\n",
    "\n",
    "if len(misclassified_indices) > 0:\n",
    "    error_pairs = {}\n",
    "    for idx in misclassified_indices:\n",
    "        true_class = class_names[test_labels[idx]]\n",
    "        pred_class = class_names[predicted_classes[idx]]\n",
    "        pair = f'{true_class} -> {pred_class}'\n",
    "        error_pairs[pair] = error_pairs.get(pair, 0) + 1\n",
    "    \n",
    "    sorted_errors = sorted(error_pairs.items(), key=lambda x: x[1], reverse=True)\n",
    "    print('\\nmost common misclassifications:')\n",
    "    for pair, count in sorted_errors[:10]:\n",
    "        print(f'{pair:30} {count:3} times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = model.count_params()\n",
    "print(f'total parameters: {total_params:,}')\n",
    "print(f'input shape: {model.input_shape}')\n",
    "print(f'output shape: {model.output_shape}')\n",
    "print(f'number of layers: {len(model.layers)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
